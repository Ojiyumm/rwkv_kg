from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS
import torch

# download models: https://huggingface.co/BlinkDL
model = RWKV(model='/home/rwkv/Peter/model/base/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth', strategy='cuda fp16')
print(model.args)
pipeline = PIPELINE(model, "rwkv_vocab_v20230424") # 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV
# use pipeline = PIPELINE(model, "rwkv_vocab_v20230424") for rwkv "world" models
states_file = '/home/rwkv/Peter/rwkv_graphrag/agents/community_report_roles/communityreport_rwkv6_7b.pth'
states = torch.load(states_file)
states_value = []
device = 'cuda'
n_head = model.args.n_head
head_size = model.args.n_embd//model.args.n_head
for i in range(model.args.n_layer):
    key = f'blocks.{i}.att.time_state'
    value = states[key]
    prev_x = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    prev_states = value.clone().detach().to(device=device,dtype=torch.float16).transpose(1,2)
    prev_ffn = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    states_value.append(prev_x)
    states_value.append(prev_states)
    states_value.append(prev_ffn)

cat_char = 'ðŸ±'
bot_char = 'ðŸ¤–'
instruction ='è¯·æ ¹æ®inputä¸­çš„æ–‡æœ¬å’Œä»»åŠ¡æè¿°ï¼Œåˆ›é€ ä¸€ä¸ªå¯ä»¥æ’°å†™ç¤¾åŒºæŠ¥å‘Šçš„ä¸“å®¶è§’è‰²ï¼Œè¿™ä¸ªä¸“å®¶éœ€è¦å’Œäººç‰©æè¿°ä¸­çš„é¢†åŸŸç›¸å…³ï¼Œå¹¶ä¸”ç”Ÿæˆä¸€ä¸ªä¸“å®¶è§’è‰²çš„ç®€å•æè¿°'
input_text = 'æœ‰ä¸ªç©ºç©ºé“äººè®¿é“æ±‚ä»™ï¼Œä»Žå¤§è’å±±æ— ç¨½å´–é’åŸ‚å³°ä¸‹ç»è¿‡ï¼Œå¿½è§ä¸€å¤§å—çŸ³ä¸Šå­—è¿¹åˆ†æ˜Žï¼Œç¼–è¿°åŽ†åŽ†ï¼Œã€ŠçŸ³å¤´è®°ã€‹æ˜¯ä¹Ÿã€‚ç©ºç©ºé“äººå°†ã€ŠçŸ³å¤´è®°ã€‹æŠ„å½•ä¸‹æ¥ï¼Œæ”¹åä¸ºã€Šæƒ…åƒ§å½•ã€‹ã€‚è‡³å´çŽ‰å³°é¢˜æ›°ã€Šçº¢æ¥¼æ¢¦ã€‹ã€‚ä¸œé²å­”æ¢…æºªåˆ™é¢˜æ›°ã€Šé£Žæœˆå®é‰´ã€‹ã€‚åŽå› æ›¹é›ªèŠ¹äºŽæ‚¼çº¢è½©ä¸­æŠ«é˜…åè½½ï¼Œå¢žåˆ äº”æ¬¡ï¼Œçº‚æˆç›®å½•ï¼Œåˆ†å‡ºç« å›žï¼Œåˆ™é¢˜æ›°ã€Šé‡‘é™µåäºŒé’—ã€‹ã€‚å§‘è‹ä¹¡å®¦ç”„å£«éšæ¢¦è§ä¸€åƒ§ä¸€é“æºæ— ç¼˜è¡¥å¤©ä¹‹çŸ³ï¼ˆé€šçµå®çŽ‰ï¼‰ä¸‹å‡¡åŽ†ç»ƒï¼Œåˆè®²ç»›ç ä»™å­ä¸ºæŠ¥ç¥žç‘›ä¾è€…æµ‡çŒä¹‹æ©è¿½éšç¥žç‘›ä¾è€…ä¸‹ä¸–ä¸ºäººï¼Œä»¥æ³ªæŠ¥æ©ã€‚æ¢¦é†’åŽï¼ŒæŠ±å¥³å„¿è‹±èŽ²åŽ»çœ‹â€œè¿‡ä¼šâ€[2]ã€‚ç”„å£«éšç»“äº¤å¹¶æŽ¥æµŽäº†å¯„å±…äºŽéš”å£è‘«èŠ¦åº™å†…çš„èƒ¡å·žäººæ°è´¾åŒ–ï¼ˆå·é›¨æ‘ï¼‰ã€‚æŸæ—¥ï¼Œè´¾é›¨æ‘é€ è®¿ç”„å£«éšï¼Œæ— æ„ä¸­é‡è§ç”„å®¶ä¸«é¬Ÿå¨‡æï¼Œä»¥ä¸ºå¨‡æå¯¹å…¶æœ‰æ„ã€‚ä¸­ç§‹æ—¶èŠ‚ï¼Œç”„å£«éšäºŽå®¶ä¸­å®´è¯·è´¾é›¨æ‘ï¼Œå¾—çŸ¥è´¾é›¨æ‘çš„æŠ±è´ŸåŽï¼Œèµ é“¶é€è¡£ä»¥ä½œè´¾é›¨æ‘ä¸Šäº¬èµ´è€ƒä¹‹ç›˜ç¼ ï¼Œç¬¬äºŒå¤©ï¼Œè´¾é›¨æ‘ä¸è¾žè€Œåˆ«ä¾¿ä¸Šè·¯èµ´è€ƒã€‚ç¬¬äºŒå¹´å…ƒå®µä½³èŠ‚å½“æ™šï¼Œç”„å®¶ä»†äººéœå¯åœ¨çœ‹ç¤¾ç«èŠ±ç¯æ—¶ï¼Œä¸æ…Žä¸¢å¤±äº†ç”„å£«éšå”¯ä¸€çš„å¥³å„¿è‹±èŽ²[3]ã€‚ä¸‰æœˆåäº”æ—¥ï¼Œè‘«èŠ¦åº™å¤±ç«ç¥¸åŠç”„å®¶ï¼Œè½é­„çš„ç”„å£«éšå¸¦å®¶äººå¯„å±…äºŽå¦‚å·žå²³ä¸ˆå°è‚ƒå®¶ä¸­ï¼ŒåŽé‡ä¸€åƒ§ä¸€é“ï¼Œæ‚Ÿå‡ºã€Šå¥½äº†æ­Œã€‹çœŸè°›ï¼Œéšåƒ§é“è€ŒåŽ»ã€‚'
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)

def my_print(s):
    print(s, end='', flush=True)



args = PIPELINE_ARGS(temperature = 1.3, top_p = 0.5, top_k = 0, # top_k = 0 then ignore
                     alpha_frequency = 0.7,
                     alpha_presence = 0.5,
                     alpha_decay = 0.996, # gradually decay the penalty
                     token_ban = [0], # ban the generation of some tokens
                     token_stop = [261], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)


pipeline.generate(ctx, token_count=200, args=args, callback=my_print,state=states_value)
print('\n')    