# This is a state for rwkv6_7b_v2.1 that generates domain, expert role in this domain and specific tasks that this export can do given input context.

* The input is solely the context that you want this model to analyze
* The output are domain, expert role in this domain and specific tasks that this export can do in a jsonl format. 

# Please refer to the following demo as test code:
```python
from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS
import torch

# download models: https://huggingface.co/BlinkDL
model = RWKV(model='/home/rwkv/Peter/model/base/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth', strategy='cuda fp16')
print(model.args)
pipeline = PIPELINE(model, "rwkv_vocab_v20230424") # 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV
# use pipeline = PIPELINE(model, "rwkv_vocab_v20230424") for rwkv "world" models
states_file = '/home/rwkv/Peter/rwkv_graphrag/agents/persona_domain_states/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth.pth'
states = torch.load(states_file)
states_value = []
device = 'cuda'
n_head = model.args.n_head
head_size = model.args.n_embd//model.args.n_head
for i in range(model.args.n_layer):
    key = f'blocks.{i}.att.time_state'
    value = states[key]
    prev_x = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    prev_states = value.clone().detach().to(device=device,dtype=torch.float16).transpose(1,2)
    prev_ffn = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    states_value.append(prev_x)
    states_value.append(prev_states)
    states_value.append(prev_ffn)

cat_char = 'ðŸ±'
bot_char = 'ðŸ¤–'
instruction ='æ ¹æ®inputä¸­çš„é¢†åŸŸå’Œä»»åŠ¡ï¼ŒååŠ©ç”¨æˆ·è¯†åˆ«inputæ–‡æœ¬ä¸­å­˜åœ¨çš„å®žä½“ç±»åž‹ã€‚ å®žä½“ç±»åž‹å¿…é¡»ä¸Žç”¨æˆ·ä»»åŠ¡ç›¸å…³ã€‚ é¿å…ä½¿ç”¨è¯¸å¦‚â€œå…¶ä»–â€æˆ–â€œæœªçŸ¥â€çš„é€šç”¨å®žä½“ç±»åž‹ã€‚ éžå¸¸é‡è¦çš„æ˜¯ï¼šä¸è¦ç”Ÿæˆå†—ä½™æˆ–é‡å çš„å®žä½“ç±»åž‹ã€‚ç”¨JSONæ ¼å¼è¾“å‡ºã€‚'
input_text = 'æœ‰ä¸ªç©ºç©ºé“äººè®¿é“æ±‚ä»™ï¼Œä»Žå¤§è’å±±æ— ç¨½å´–é’åŸ‚å³°ä¸‹ç»è¿‡ï¼Œå¿½è§ä¸€å¤§å—çŸ³ä¸Šå­—è¿¹åˆ†æ˜Žï¼Œç¼–è¿°åŽ†åŽ†ï¼Œã€ŠçŸ³å¤´è®°ã€‹æ˜¯ä¹Ÿã€‚ç©ºç©ºé“äººå°†ã€ŠçŸ³å¤´è®°ã€‹æŠ„å½•ä¸‹æ¥ï¼Œæ”¹åä¸ºã€Šæƒ…åƒ§å½•ã€‹ã€‚è‡³å´çŽ‰å³°é¢˜æ›°ã€Šçº¢æ¥¼æ¢¦ã€‹ã€‚ä¸œé²å­”æ¢…æºªåˆ™é¢˜æ›°ã€Šé£Žæœˆå®é‰´ã€‹ã€‚åŽå› æ›¹é›ªèŠ¹äºŽæ‚¼çº¢è½©ä¸­æŠ«é˜…åè½½ï¼Œå¢žåˆ äº”æ¬¡ï¼Œçº‚æˆç›®å½•ï¼Œåˆ†å‡ºç« å›žï¼Œåˆ™é¢˜æ›°ã€Šé‡‘é™µåäºŒé’—ã€‹ã€‚å§‘è‹ä¹¡å®¦ç”„å£«éšæ¢¦è§ä¸€åƒ§ä¸€é“æºæ— ç¼˜è¡¥å¤©ä¹‹çŸ³ï¼ˆé€šçµå®çŽ‰ï¼‰ä¸‹å‡¡åŽ†ç»ƒï¼Œåˆè®²ç»›ç ä»™å­ä¸ºæŠ¥ç¥žç‘›ä¾è€…æµ‡çŒä¹‹æ©è¿½éšç¥žç‘›ä¾è€…ä¸‹ä¸–ä¸ºäººï¼Œä»¥æ³ªæŠ¥æ©ã€‚æ¢¦é†’åŽï¼ŒæŠ±å¥³å„¿è‹±èŽ²åŽ»çœ‹â€œè¿‡ä¼šâ€[2]ã€‚ç”„å£«éšç»“äº¤å¹¶æŽ¥æµŽäº†å¯„å±…äºŽéš”å£è‘«èŠ¦åº™å†…çš„èƒ¡å·žäººæ°è´¾åŒ–ï¼ˆå·é›¨æ‘ï¼‰ã€‚æŸæ—¥ï¼Œè´¾é›¨æ‘é€ è®¿ç”„å£«éšï¼Œæ— æ„ä¸­é‡è§ç”„å®¶ä¸«é¬Ÿå¨‡æï¼Œä»¥ä¸ºå¨‡æå¯¹å…¶æœ‰æ„ã€‚ä¸­ç§‹æ—¶èŠ‚ï¼Œç”„å£«éšäºŽå®¶ä¸­å®´è¯·è´¾é›¨æ‘ï¼Œå¾—çŸ¥è´¾é›¨æ‘çš„æŠ±è´ŸåŽï¼Œèµ é“¶é€è¡£ä»¥ä½œè´¾é›¨æ‘ä¸Šäº¬èµ´è€ƒä¹‹ç›˜ç¼ ï¼Œç¬¬äºŒå¤©ï¼Œè´¾é›¨æ‘ä¸è¾žè€Œåˆ«ä¾¿ä¸Šè·¯èµ´è€ƒã€‚ç¬¬äºŒå¹´å…ƒå®µä½³èŠ‚å½“æ™šï¼Œç”„å®¶ä»†äººéœå¯åœ¨çœ‹ç¤¾ç«èŠ±ç¯æ—¶ï¼Œä¸æ…Žä¸¢å¤±äº†ç”„å£«éšå”¯ä¸€çš„å¥³å„¿è‹±èŽ²[3]ã€‚ä¸‰æœˆåäº”æ—¥ï¼Œè‘«èŠ¦åº™å¤±ç«ç¥¸åŠç”„å®¶ï¼Œè½é­„çš„ç”„å£«éšå¸¦å®¶äººå¯„å±…äºŽå¦‚å·žå²³ä¸ˆå°è‚ƒå®¶ä¸­ï¼ŒåŽé‡ä¸€åƒ§ä¸€é“ï¼Œæ‚Ÿå‡ºã€Šå¥½äº†æ­Œã€‹çœŸè°›ï¼Œéšåƒ§é“è€ŒåŽ»ã€‚'
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)

def my_print(s):
    print(s, end='', flush=True)



args = PIPELINE_ARGS(temperature = 1, top_p = 0.2, top_k = 0, # top_k = 0 then ignore
                     alpha_frequency = 0.5,
                     alpha_presence = 0.5,
                     alpha_decay = 0.998, # gradually decay the penalty
                     token_ban = [0], # ban the generation of some tokens
                     token_stop = [0,1], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)

pipeline.generate(ctx, token_count=1000, args=args, callback=my_print,state=states_value)
print('\n')
```    
# The final printed input and output:
![](./persona_domain_demo.png) 