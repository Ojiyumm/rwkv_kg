from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS
import torch

# download models: https://huggingface.co/BlinkDL
model = RWKV(model='/home/rwkv/Peter/model/base/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth', strategy='cuda fp16')
print(model.args)
pipeline = PIPELINE(model, "rwkv_vocab_v20230424") # 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV
# use pipeline = PIPELINE(model, "rwkv_vocab_v20230424") for rwkv "world" models
states_file = '/home/rwkv/Peter/rwkv_graphrag/agents/entity_type_extraction/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth.pth'
states = torch.load(states_file)
states_value = []
device = 'cuda'
n_head = model.args.n_head
head_size = model.args.n_embd//model.args.n_head
for i in range(model.args.n_layer):
    key = f'blocks.{i}.att.time_state'
    value = states[key]
    prev_x = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    prev_states = value.clone().detach().to(device=device,dtype=torch.float16).transpose(1,2)
    prev_ffn = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    states_value.append(prev_x)
    states_value.append(prev_states)
    states_value.append(prev_ffn)

cat_char = 'ðŸ±'
bot_char = 'ðŸ¤–'
instruction ='æ ¹æ®inputä¸­çš„é¢†åŸŸå’Œä»»åŠ¡ï¼ŒååŠ©ç”¨æˆ·è¯†åˆ«inputæ–‡æœ¬ä¸­å­˜åœ¨çš„å®žä½“ç±»åž‹ã€‚ å®žä½“ç±»åž‹å¿…é¡»ä¸Žç”¨æˆ·ä»»åŠ¡ç›¸å…³ã€‚ é¿å…ä½¿ç”¨è¯¸å¦‚â€œå…¶ä»–â€æˆ–â€œæœªçŸ¥â€çš„é€šç”¨å®žä½“ç±»åž‹ã€‚ éžå¸¸é‡è¦çš„æ˜¯ï¼šä¸è¦ç”Ÿæˆå†—ä½™æˆ–é‡å çš„å®žä½“ç±»åž‹ã€‚ç”¨JSONæ ¼å¼è¾“å‡ºã€‚'
input_text = '{"é¢†åŸŸ": "æ–‡å­¦ä¸Žç¥žè¯", "ä¸“å®¶": "æ–‡å­¦å²å­¦è€…/ç¥žè¯å­¦å®¶", "ä»»åŠ¡": ["åˆ†æžã€ŠçŸ³å¤´è®°ã€‹çš„åŽ†å²èƒŒæ™¯å’Œå½±å“", "ç ”ç©¶ã€Šçº¢æ¥¼æ¢¦ã€‹ä¸Žã€Šé‡‘é™µåäºŒé’—ã€‹ä¹‹é—´çš„å…³ç³»", "æŽ¢è®¨ä¸œé²å­”æ¢…æºªå¯¹ã€ŠçŸ³å¤´è®°ã€‹çš„æ”¹ç¼–è¿‡ç¨‹", "è§£æžå´çŽ‰å³°åœ¨ã€Šçº¢æ¥¼æ¢¦ã€‹ä¸­çš„è§’è‰²å’Œè´¡çŒ®", "è¯„ä¼°æ›¹é›ªèŠ¹åœ¨ã€Šæ‚¼çº¢è½©ä¸­æŠ«é˜…åäº”é—´ã€‹ä¸­çš„å†™ä½œæŠ€å·§"]}'
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)

def my_print(s):
    print(s, end='', flush=True)



args = PIPELINE_ARGS(temperature = 1, top_p = 0.2, top_k = 0, # top_k = 0 then ignore
                     alpha_frequency = 0.5,
                     alpha_presence = 0.5,
                     alpha_decay = 0.998, # gradually decay the penalty
                     token_ban = [0], # ban the generation of some tokens
                     token_stop = [0,1], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)

pipeline.generate(ctx, token_count=200, args=args, callback=my_print,state=states_value)
print('\n')    