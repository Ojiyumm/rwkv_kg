from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS
import torch

# download models: https://huggingface.co/BlinkDL
model = RWKV(model='/home/rwkv/Peter/model/base/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth', strategy='cuda fp16')
print(model.args)
pipeline = PIPELINE(model, "rwkv_vocab_v20230424") # 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV
# use pipeline = PIPELINE(model, "rwkv_vocab_v20230424") for rwkv "world" models
states_file = '/home/rwkv/Peter/rwkv_graphrag/agents/entity_summary/entity_summary.pth'
states = torch.load(states_file)
states_value = []
device = 'cuda'
n_head = model.args.n_head
head_size = model.args.n_embd//model.args.n_head
for i in range(model.args.n_layer):
    key = f'blocks.{i}.att.time_state'
    value = states[key]
    prev_x = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    prev_states = value.clone().detach().to(device=device,dtype=torch.float16).transpose(1,2)
    prev_ffn = torch.zeros(model.args.n_embd,device=device,dtype=torch.float16)
    states_value.append(prev_x)
    states_value.append(prev_states)
    states_value.append(prev_ffn)

cat_char = 'ðŸ±'
bot_char = 'ðŸ¤–'
instruction ='è¯·é˜…è¯»iputä¸­çš„entitieså’Œdescritionsï¼Œå›´ç»•entityå’Œdescritionå†™ä¸€ä¸ªç®€å•çš„200å­—ä»‹ç»ï¼Œä»‹ç»éœ€è¦åŒ…æ‹¬æ‰€çš„entityå’Œä»–ä»¬ä¹‹é—´çš„å…³ç³».æœ€ç»ˆå†…å®¹ä¸èƒ½è¶…è¿‡200å­—'
input_text = '"entities": ["æ±‰è¯­", "è¯­ä¹‰åç§»", "æž„å¼è¯­æ³•", "è¯„ä»·æ€§è¯­å¢ƒ", "è¯æ±‡æ„ä¹‰"], "descriptions": ["æ±‰è¯­æ˜¯ä¸­å›½çš„ä¸»è¦è¯­è¨€ï¼Œå…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰ç»“æž„å’Œå¤æ‚çš„è¯­æ³•ä½“ç³»ã€‚", "è¯­ä¹‰åç§»æ˜¯æŒ‡åœ¨ç‰¹å®šè¯­å¢ƒä¸‹ï¼Œè¯è¯­çš„æ„ä¹‰å‘ç”Ÿçš„å˜åŒ–æˆ–åç¦»ã€‚", "æž„å¼è¯­æ³•ç ”ç©¶çš„æ˜¯å¥å­ç»“æž„çš„æ¨¡å¼åŠå…¶åŠŸèƒ½ï¼Œæ˜¯è¯­è¨€å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚", "è¯„ä»·æ€§è¯­å¢ƒæŒ‡çš„æ˜¯åŒ…å«æƒ…æ„Ÿè‰²å½©æˆ–è¯„ä»·æ€§è´¨çš„è¯­è¨€çŽ¯å¢ƒï¼Œå½±å“ç€è¯­è¨€è¡¨è¾¾çš„æ„ä¹‰ã€‚", "è¯æ±‡æ„ä¹‰æŒ‡çš„æ˜¯å•è¯åœ¨ç‰¹å®šè¯­å¢ƒä¸‹çš„å…·ä½“å«ä¹‰ï¼Œå¯ä»¥å› è¯­å¢ƒè€Œå˜åŒ–ã€‚"]'
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)

def my_print(s):
    print(s, end='', flush=True)



args = PIPELINE_ARGS(temperature = 1.3, top_p = 0.5, top_k = 0, # top_k = 0 then ignore
                     alpha_frequency = 0.7,
                     alpha_presence = 0.5,
                     alpha_decay = 0.996, # gradually decay the penalty
                     token_ban = [0], # ban the generation of some tokens
                     token_stop = [bot_char], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)

pipeline.generate(ctx, token_count=200, args=args, callback=my_print,state=states_value)
print('\n')    