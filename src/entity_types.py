import json
import torch
import argparse

from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS





def generate_persona_domain(model_path, states_file, input_text, token_count=2000):
    # Load the model
    model = RWKV(model=model_path, strategy='cuda fp16')
    print("Loading persona states")

    # Initialize the pipeline
    pipeline = PIPELINE(model, "rwkv_vocab_v20230424")

    # Load previous states
    states = torch.load(states_file)
    states_value = []
    device = 'cuda'
    

    for i in range(model.args.n_layer):
        key = f'blocks.{i}.att.time_state'
        value = states[key]
        prev_x = torch.zeros(model.args.n_embd, device=device, dtype=torch.float16)
        prev_states = value.clone().detach().to(device=device, dtype=torch.float16).transpose(1, 2)
        prev_ffn = torch.zeros(model.args.n_embd, device=device, dtype=torch.float16)
        states_value.append(prev_x)
        states_value.append(prev_states)
        states_value.append(prev_ffn)

    # Create the context for the model
    cat_char = 'ğŸ±'
    bot_char = 'ğŸ¤–'
    instruction ='æ ¹æ®inputä¸­æ–‡æœ¬å†…å®¹ï¼ŒååŠ©ç”¨æˆ·è¯†åˆ«æ–‡æœ¬æ‰€å±çš„é¢†åŸŸã€‚éšåï¼Œæ‰¾å‡ºä¸è¯¥é¢†åŸŸå…³è”æœ€ç´§å¯†çš„ä¸“å®¶ã€‚æ¥ç€ï¼Œä½œä¸ºè¾“å‡ºï¼Œåˆ—ä¸¾å‡ºäº”è‡³åé¡¹å¯åœ¨è¯¥æ–‡æœ¬ä¸­æ‰§è¡Œçš„å…·ä½“ä»»åŠ¡ã€‚æ¥ä¸‹æ¥ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼šé¢†åŸŸï¼šå¯¹äºç»™å®šçš„ç¤ºä¾‹æ–‡æœ¬ï¼Œå¸®åŠ©ç”¨æˆ·æŒ‡å®šä¸€ä¸ªæè¿°æ€§é¢†åŸŸï¼Œæ¦‚æ‹¬æ–‡æœ¬çš„ä¸»é¢˜ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›ç­”ï¼Œæ— æ³•æå–åˆ™ä¸è¾“å‡º'
    ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
    

    

    # Set up the generation arguments
    args = PIPELINE_ARGS(temperature = 1, top_p = 0.2, top_k = 0, # top_k = 0 then ignore
                     alpha_frequency = 0.5,
                     alpha_presence = 0.5,
                     alpha_decay = 0.998, # gradually decay the penalty
                     token_ban = [0], # ban the generation of some tokens
                     token_stop = [0,1], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)

    # Generate the response
    persona_domain=pipeline.generate(ctx, token_count=token_count, args=args, callback=my_print, state=states_value)
    
    return persona_domain



def generate_entity_types(model_path, states_file, input_text, token_count=2000):
    torch.cuda.empty_cache()
    # Load the model
    model = RWKV(model=model_path, strategy='cuda fp16')
    print('Loading Entity Types States')

    # Initialize the pipeline
    pipeline = PIPELINE(model, "rwkv_vocab_v20230424")

    # Load previous states
    states = torch.load(states_file)
    states_value = []
    device = 'cuda'
    

    for i in range(model.args.n_layer):
        key = f'blocks.{i}.att.time_state'
        value = states[key]
        prev_x = torch.zeros(model.args.n_embd, device=device, dtype=torch.float16)
        prev_states = value.clone().detach().to(device=device, dtype=torch.float16).transpose(1, 2)
        prev_ffn = torch.zeros(model.args.n_embd, device=device, dtype=torch.float16)
        states_value.append(prev_x)
        states_value.append(prev_states)
        states_value.append(prev_ffn)

    # Create the context for the model
    cat_char = 'ğŸ±'
    bot_char = 'ğŸ¤–'
    instruction ='æ ¹æ®inputä¸­çš„é¢†åŸŸå’Œä»»åŠ¡ï¼ŒååŠ©ç”¨æˆ·è¯†åˆ«inputæ–‡æœ¬ä¸­å­˜åœ¨çš„å®ä½“ç±»å‹ã€‚ å®ä½“ç±»å‹å¿…é¡»ä¸ç”¨æˆ·ä»»åŠ¡ç›¸å…³ã€‚ é¿å…ä½¿ç”¨è¯¸å¦‚â€œå…¶ä»–â€æˆ–â€œæœªçŸ¥â€çš„é€šç”¨å®ä½“ç±»å‹ã€‚ éå¸¸é‡è¦çš„æ˜¯ï¼šä¸è¦ç”Ÿæˆå†—ä½™æˆ–é‡å çš„å®ä½“ç±»å‹ã€‚ç”¨JSONæ ¼å¼è¾“å‡ºã€‚'
    ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
    

    

    # Set up the generation arguments
    args = PIPELINE_ARGS(temperature = 1, top_p = 0.2, top_k = 0, # top_k = 0 then ignore
                     alpha_frequency = 0.5,
                     alpha_presence = 0.5,
                     alpha_decay = 0.998, # gradually decay the penalty
                     token_ban = [0], # ban the generation of some tokens
                     token_stop = [0,1], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)

    # Generate the response
    entity_types=pipeline.generate(ctx, token_count=token_count, args=args, callback=my_print, state=states_value)
    
    return entity_types




def format_input_and_entities(input_text, output_path,entity_types):
    # Create the structured data
    structured_data = {
        "input": input_text
    }
    
    # Convert to JSON string
    input_text_formatted = f'{structured_data},{entity_types}'
    
    with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(input_text_formatted, f, ensure_ascii=False)
            f.write('\n') 
    return input_text_formatted

def my_print(s):
        print(s, end='', flush=True)





if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Generate persona domain and entity types from input text.')
    parser.add_argument('--input_text', type=str, required=False, default='æœ‰ä¸ªç©ºç©ºé“äººè®¿é“æ±‚ä»™ï¼Œä»å¤§è’å±±æ— ç¨½å´–é’åŸ‚å³°ä¸‹ç»è¿‡ï¼Œå¿½è§ä¸€å¤§å—çŸ³ä¸Šå­—è¿¹åˆ†æ˜ï¼Œç¼–è¿°å†å†ï¼Œã€ŠçŸ³å¤´è®°ã€‹æ˜¯ä¹Ÿã€‚ç©ºç©ºé“äººå°†ã€ŠçŸ³å¤´è®°ã€‹æŠ„å½•ä¸‹æ¥ï¼Œæ”¹åä¸ºã€Šæƒ…åƒ§å½•ã€‹ã€‚è‡³å´ç‰å³°é¢˜æ›°ã€Šçº¢æ¥¼æ¢¦ã€‹ã€‚ä¸œé²å­”æ¢…æºªåˆ™é¢˜æ›°ã€Šé£æœˆå®é‰´ã€‹ã€‚åå› æ›¹é›ªèŠ¹äºæ‚¼çº¢è½©ä¸­æŠ«é˜…åè½½ï¼Œå¢åˆ äº”æ¬¡ï¼Œçº‚æˆç›®å½•ï¼Œåˆ†å‡ºç« å›ï¼Œåˆ™é¢˜æ›°ã€Šé‡‘é™µåäºŒé’—ã€‹ã€‚å§‘è‹ä¹¡å®¦ç”„å£«éšæ¢¦è§ä¸€åƒ§ä¸€é“æºæ— ç¼˜è¡¥å¤©ä¹‹çŸ³ï¼ˆé€šçµå®ç‰ï¼‰ä¸‹å‡¡å†ç»ƒï¼Œåˆè®²ç»›ç ä»™å­ä¸ºæŠ¥ç¥ç‘›ä¾è€…æµ‡çŒä¹‹æ©è¿½éšç¥ç‘›ä¾è€…ä¸‹ä¸–ä¸ºäººï¼Œä»¥æ³ªæŠ¥æ©ã€‚æ¢¦é†’åï¼ŒæŠ±å¥³å„¿è‹±è²å»çœ‹â€œè¿‡ä¼šâ€[2]ã€‚ç”„å£«éšç»“äº¤å¹¶æ¥æµäº†å¯„å±…äºéš”å£è‘«èŠ¦åº™å†…çš„èƒ¡å·äººæ°è´¾åŒ–ï¼ˆå·é›¨æ‘ï¼‰ã€‚æŸæ—¥ï¼Œè´¾é›¨æ‘é€ è®¿ç”„å£«éšï¼Œæ— æ„ä¸­é‡è§ç”„å®¶ä¸«é¬Ÿå¨‡æï¼Œä»¥ä¸ºå¨‡æå¯¹å…¶æœ‰æ„ã€‚ä¸­ç§‹æ—¶èŠ‚ï¼Œç”„å£«éšäºå®¶ä¸­å®´è¯·è´¾é›¨æ‘ï¼Œå¾—çŸ¥è´¾é›¨æ‘çš„æŠ±è´Ÿåï¼Œèµ é“¶é€è¡£ä»¥ä½œè´¾é›¨æ‘ä¸Šäº¬èµ´è€ƒä¹‹ç›˜ç¼ ï¼Œç¬¬äºŒå¤©ï¼Œè´¾é›¨æ‘ä¸è¾è€Œåˆ«ä¾¿ä¸Šè·¯èµ´è€ƒã€‚ç¬¬äºŒå¹´å…ƒå®µä½³èŠ‚å½“æ™šï¼Œç”„å®¶ä»†äººéœå¯åœ¨çœ‹ç¤¾ç«èŠ±ç¯æ—¶ï¼Œä¸æ…ä¸¢å¤±äº†ç”„å£«éšå”¯ä¸€çš„å¥³å„¿è‹±è²[3]ã€‚ä¸‰æœˆåäº”æ—¥ï¼Œè‘«èŠ¦åº™å¤±ç«ç¥¸åŠç”„å®¶ï¼Œè½é­„çš„ç”„å£«éšå¸¦å®¶äººå¯„å±…äºå¦‚å·å²³ä¸ˆå°è‚ƒå®¶ä¸­ï¼Œåé‡ä¸€åƒ§ä¸€é“ï¼Œæ‚Ÿå‡ºã€Šå¥½äº†æ­Œã€‹çœŸè°›ï¼Œéšåƒ§é“è€Œå»ã€‚')
    parser.add_argument('--output_path', type=str, required=False, default='/home/rwkv/Peter/rwkv_graphrag/data/triple_input.jsonl')
    parser.add_argument('--base_model', type=str, required=False, default='/home/rwkv/Peter/models/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth')
    parser.add_argument('--persona_domain_state', type=str, required=False, default='/home/rwkv/Peter/rwkv_graphrag/agents/persona_domain_states/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth.pth')
    parser.add_argument('--entity_types_state', type=str, required=False, default='/home/rwkv/Peter/rwkv_graphrag/agents/entity_type_extraction/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth.pth')
    parser.add_argument('--token_count', type=int, default=2000, help='Token count for generation')

    args = parser.parse_args()

    # Generate persona domain
    persona = generate_persona_domain(model_path=args.base_model, states_file=args.persona_domain_state, input_text=args.input_text)
    torch.cuda.empty_cache()

    # Generate entity types
    entity_types = generate_entity_types(model_path=args.base_model, states_file=args.entity_types_state, input_text=persona)
    torch.cuda.empty_cache()

    # Format input and entities
    triple_input = format_input_and_entities(input_text=args.input_text, output_path=args.output_path,entity_types=entity_types)

    print(triple_input)






